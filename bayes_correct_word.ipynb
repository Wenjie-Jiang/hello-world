{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Introduction\n",
    "\n",
    "Nowaday, people seem to be more accustomed to typing by hand, whether at work or in life, rather than writing by hand. Compared to the pen and paper, the 26-key input method clearly occupies a larger part of the lives of many people. As a result, the previous typos have become errors in the document. How to correctly identify these errors and correct them intelligently becomes a feature we urgently need. Now our commonly used document editing software has introduced the function of typo self-correction. For example, when you type \"teh\", \"Word\" will automatically change it to \"the\". When you type \"speling\" with Google search, in less than 0.1 seconds, Google will return: What you are looking for is \"spelling\". (Yahoo! and Microsoft have similar features). Of course, this is the simplest example. How can a computer intelligently find a document error and correct it under normal circumstances ?\n",
    "\n",
    "In fact, what we usually call a typo can be broken down into two kinds of mistakes.\n",
    "\n",
    "1. None-word error\n",
    "That is, the wrongly written words are not the existing words in the dictionary, such as creation written as creaton. This kind of error is very well recognized, just need to have a pre-loaded dictionary.\n",
    "\n",
    "2. Real-word error\n",
    "As the name implies, this kind of error refers to the existence of the wrong word in the dictionary. Relatively speaking, this kind of error is more difficult to identify. Simple examples such as the result of losing three.\n",
    "\n",
    "We will describe how to make automatic corrections after the wrong words are identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working principle\n",
    "\n",
    "Given a word, our task is to choose the correct spelling word that is most similar to it. (If the word itself is spelled correctly, then the closest thing is it.) Of course, it is impossible to find similar words absolutely. For example, given the word \"laters\", should it be corrected for \"late\" or \"latest\"? These difficulties indicate that we need to use probability theory instead of rule-based judgment. We say, given a word w, we want to find a correct word c in all correct words database, so that the conditional probability for w is the largest, that is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$argmax_{c∈candidates} \\,\\, p(c|w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Bayesian theory, the above equation is equivalent to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$argmax_{c∈candidates}  \\,\\, \\frac{P(c)×P(w|c)}{P(w)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $P(w)$ is the same for each c to be selected, we ignore this factor and the final formula is transformed into:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$argmax_{c∈candidates}  \\,\\, P(c)×P(w|c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four main parts in this formula:\n",
    "\n",
    "1. Selection agency: $argmax$ <br>\n",
    "    We choose the most probable word in the alternative word as the output\n",
    "2. Alternative model: $c∈candidates$ <br>\n",
    "    This section tells us which words to consider as alternatives.\n",
    "3. Language model: $P(c)$ <br>\n",
    "    The probability that the word c appears in the corpus. For example, in an English corpus, 7% of the words are \"the\", then P(the)=0.07\n",
    "4. Error model P(w|c) <br>\n",
    "When the user wants to input c, the probability of inputting as w is wrong. For example, P(teh|the) should be much larger than P(theeexyz|the).\n",
    "\n",
    "We replace the delay probability $P(c|w)$ with the conditional probability $P(w|c)$ and the prior probability $P(c)$, which are easy to consider and solve, so that the problem is easier to analyze and solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first is to calculate $P(c)$. We can read in a huge text file, [big.txt](http://www.norvig.com/big.txt), which has about a few million words (equivalent to a corpus). This file is composed of some books that can be obtained from the Gutenberg project, [Wiktionary](https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists) and [British National Corpus](http://www.kilgarriff.co.uk/bnc-readme.html) corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/revincent/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3911095\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# count the number of each word\n",
    "from collections import Counter\n",
    "from nltk.corpus import gutenberg\n",
    "# natural language processing package\n",
    "import nltk\n",
    "import collections\n",
    "import re\n",
    "\n",
    "def load_words(file_name):\n",
    "    word = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        for line in f:\n",
    "            # split words by utilizing nltk.word_tokenize() function\n",
    "            word += nltk.word_tokenize(line)\n",
    "    return word\n",
    "\n",
    "#download gutenberg corpus\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "# obtain Corpus for priori probability c\n",
    "Corpus = load_words(\"big.txt\") + gutenberg.words()\n",
    "\n",
    "# it's a dictionary and obtain the number of each word\n",
    "wordDict = Counter(Corpus)\n",
    "\n",
    "# obtain the words set including all of words, use for checking \n",
    "# whether a word is in the set or not.\n",
    "wordSet = set(wordDict.keys())\n",
    "\n",
    "print(len(Corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, wordSet stores how many times each word appears in the corpus. But one question is what to do if you encounter a new word we have never seen before. If a word is spelled exactly right, but the word is not included in the corpus, the word will never appear in the training set. So, we have to return the probability that the word appears to be 0. This situation is not very good, because the probability 0 means that this event is absolutely impossible. In our probability model, we expect to represent this situation with a small probability. In fact, there are many standard methods for forming this problem. We choose one of the most simple method: New words that have never been seen are assumed to have occurred once. This process generally becomes \"smoothing\", because we set the probability distribution to 0 to a small probability value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the question is: Given a word w, how can I enumerate all possible correct spellings? In fact, the predecessors have studied very well, this is a concept of \"editing distance\". Editing distance between two words: defined as the use of insertion (insert a single letter in a word), deletion (delete a single letter), transposition (exchange two adjacent letters), substitution (change one letter to another) to form another word.\n",
    "The following function returns all collections with a editing distance of 1 from the word w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance_1(word):\n",
    "    # candidate alphabet\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    n = len(word)\n",
    "    \n",
    "    # delete operation\n",
    "    deletion = []\n",
    "    for i in range(n):\n",
    "        # delete the ith alphabet in word\n",
    "        deletion.append(word[0:i] + word[i + 1:])\n",
    "        \n",
    "    # replace operation\n",
    "    substitution = []\n",
    "    for i in range(n):\n",
    "        for c in alphabet:\n",
    "            # use alphabet c to replace the ith alphabet in word\n",
    "            substitution.append(word[0:i] + c + word[i+1:])\n",
    "            \n",
    "    # insert operation\n",
    "    insertion = []\n",
    "    for i in range(n+1):\n",
    "        for c in alphabet:\n",
    "            # insert alphabet c to the ith position in word\n",
    "            insertion.append(word[0:i] + c + word[i:])\n",
    "            \n",
    "    # exchange operation\n",
    "    transposition = []\n",
    "    for i in range(n-1):\n",
    "        # exchange ith and (i+1)th position\n",
    "        transposition.append(word[0:i] + word[i+1] + word[i] + word[i+2:])\n",
    "    \n",
    "    # delete repeation\n",
    "    edit1_words = set(deletion + substitution + insertion + transposition)\n",
    "    \n",
    "    return edit1_words\n",
    "\n",
    "# print(len(edit_distance_1(\"something\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, this collection is very big. For a word of length n, there may be n deletions, n-1 swaps, 26n substitutions, and 26(n+1) insertions.(There is a bit of repetition). For example, there are 511 words which generated by \"something\" with editing distance of 1, but it is actually 494.\n",
    "\n",
    "However, we can define a module that identifies the correctness of these generated alternative words, matching only the words that exist in the dictionary. This set will become very small, because many of the randomly generated words are illegally spelled and not really exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \n",
    "    # Before optimization\n",
    "    '''\n",
    "    known_words = []\n",
    "    for w in words:\n",
    "        # if w in Corpus(wordSet), that means w is known word, else is unknown word.\n",
    "        if w in wordSet:\n",
    "            known_words.append(w)\n",
    "    \n",
    "    return set(known_words)\n",
    "    '''\n",
    "    \n",
    "    # After optimization\n",
    "    # Use the intersection operation of sets with python to accelerate\n",
    "    \n",
    "    return words & wordSet\n",
    "\n",
    "# print(len(known(edit_distance_1(\"something\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spell-checking literature claims that approximately 80-95% of spelling errors are within editing distance of 1. However, when we experimented with a 270 misspelled corpus, we found that only 76% of misspellings belonged to a set with an editing distance of 1. So we started thinking about editing those words with a distance of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def edit_distance_2(word):\n",
    "    \n",
    "    edit2_words = []\n",
    "    # obtain editing distance of 1 and known words, reduce the time complexity\n",
    "    # and then use edit_distance_1() function editing again, will obtain \n",
    "    # editing distance of 2.\n",
    "    edit1_words = known(edit_distance_1(word))\n",
    "    for e1 in edit1_words:\n",
    "        edit2 = known(edit_distance_1(e1))\n",
    "        edit2_words += edit2\n",
    "    \n",
    "    # delete repeation\n",
    "    \n",
    "    return set(edit2_words)\n",
    "\n",
    "print(len(edit_distance_2(\"something\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing left is the error model part $P(w|c)$. Choose a simple method:\n",
    "1. Editing the correct word with a distance of 1 is higher than the editing distance of 2,\n",
    "2. The correct word priority with an edit distance of 0 is higher than the edit distance of 1.\n",
    "<br>Therefore, writing it out in code is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "# according to priority definition to select candidate words\n",
    "def candidate_word(word):\n",
    "    \n",
    "    candidates = known(set([word])) or known(edit_distance_1(word)) \\\n",
    "    or known(edit_distance_2(word)) or set([word])\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "# according to the candidate words, select the maximum probability as correct word\n",
    "def correct_word(word):\n",
    "    \n",
    "    # The number of occurrences of a word as a basis for sorting\n",
    "#     print(candidate_word(word))\n",
    "    return max(candidate_word(word), key=lambda w: wordDict[w])\n",
    "\n",
    "print(correct_word(\"gruop\"))\n",
    "print(correct_word(\"teh\"))\n",
    "\n",
    "# will output \"the\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "## Performance\n",
    "Roger Mitton's Birkbeck misspelling corpus was downloaded from the [Oxford Text Archive](http://ota.ahds.ac.uk/texts/0643.html). There are 2 misspelling sets to be used for testing the accuracy of the model and time cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "# check if the word contains illegal characters\n",
    "def isvalid(word): \n",
    "    \n",
    "    '''include numbers'''\n",
    "    if bool(re.search(r'\\d', word)) == True:\n",
    "        return False\n",
    "    \n",
    "    '''inclue - '''\n",
    "    if bool(\"-\" in word) == True:\n",
    "        return False\n",
    "    \n",
    "    '''include * '''\n",
    "    if bool(\"*\" in word) == True:\n",
    "        return False\n",
    "    \n",
    "    '''include ' ''' \n",
    "    if bool(\"'\" in word) == True:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def load_spellerrors1(file_name):\n",
    "    word_dict = {}\n",
    "    with open(file_name) as fr:\n",
    "        for line in fr:\n",
    "            splited = line.strip(\" \").strip(\"\\n\").split(\":\")\n",
    "            correct, wrong = splited[0], [x.strip(\" \") for x in splited[1].split(\" \")]\n",
    "            word_dict[correct] = wrong[1:]\n",
    "    \n",
    "    return word_dict\n",
    "\n",
    "def load_spellerrors2(file_name):\n",
    "    word_dict = {}\n",
    "    total_line = 0\n",
    "    with open(file_name) as fr:\n",
    "        for line in fr:\n",
    "            # due to autoencorrect.spell() spend too much time to correct, \n",
    "            # only choose 200 lines data\n",
    "            if total_line > 200:\n",
    "                break\n",
    "            total_line += 1\n",
    "            splited = line.strip(\" \").strip(\"\\n\").split(\":\")\n",
    "            correct, wrong = splited[0], [x.strip(\" \") for x in splited[1].split(\",\")]\n",
    "            if isvalid(correct) == False:\n",
    "                continue\n",
    "            word_dict[correct] = []\n",
    "            for w in wrong:\n",
    "                if isvalid(w) == True:\n",
    "                    word_dict[correct].append(w)\n",
    "#             print(correct, word_dict[correct])\n",
    "    \n",
    "    return word_dict\n",
    "\n",
    "words_1 = load_spellerrors1(\"spell-errors1.txt\")\n",
    "words_2 = load_spellerrors2(\"spell-errors.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_accuracy(word_dict):\n",
    "    \n",
    "    begin_time = time.time()\n",
    "    correct_num = 0\n",
    "    unknown_num = 0\n",
    "    total_num = 0\n",
    "    for correct, wrong in word_dict.items():\n",
    "        for word in wrong:\n",
    "#             print(correct, wrong)\n",
    "            total_num += 1\n",
    "            \n",
    "            # if the word is unknown\n",
    "            if correct not in wordSet:\n",
    "                unknown_num += 1\n",
    "                continue\n",
    "                \n",
    "            target = correct_word(word)\n",
    "            if target == correct:\n",
    "                correct_num += 1\n",
    "                \n",
    "    accuracy = correct_num * 1.0 / total_num\n",
    "    end_time = time.time()\n",
    "    spend_time = end_time - begin_time\n",
    "    \n",
    "    print(\"correct words: \", correct_num)\n",
    "    print(\"unknown words: \", unknown_num)\n",
    "    print(\"total word: \", total_num)\n",
    "    print(\"spend time: \", spend_time, \"seconds\")\n",
    "    print(\"accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct words:  164\n",
      "unknown words:  10\n",
      "total word:  270\n",
      "spend time:  0.06961774826049805 seconds\n",
      "accuracy:  0.6074074074074074\n"
     ]
    }
   ],
   "source": [
    "test_accuracy(words_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct words:  194\n",
      "unknown words:  35\n",
      "total word:  823\n",
      "spend time:  0.22311997413635254 seconds\n",
      "accuracy:  0.23572296476306198\n"
     ]
    }
   ],
   "source": [
    "test_accuracy(words_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Study\n",
    "\n",
    "[Autocorrect](https://pypi.org/project/autocorrect/0.1.0/) is an open source library for Python that automatically corrects spelling errors in toolkits. We use the functions in this package to test the test set and compare the results with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from autocorrect import spell\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def auto_correct(word_dict):\n",
    "    \n",
    "    begin_time = time.time()\n",
    "    correct_num = 0\n",
    "#     unknown_num = 0\n",
    "    total_num = 0\n",
    "    for correct, wrong in word_dict.items():\n",
    "        for word in wrong:\n",
    "            total_num += 1\n",
    "            \n",
    "            # if the word is unknown\n",
    "#             if correct not in wordSet:\n",
    "#                 unknown_num += 1\n",
    "#                 continue\n",
    "                \n",
    "            target = spell(word)\n",
    "            if target == correct:\n",
    "                correct_num += 1\n",
    "                \n",
    "    accuracy = correct_num * 1.0 / total_num\n",
    "    end_time = time.time()\n",
    "    spend_time = end_time - begin_time\n",
    "    \n",
    "    print(\"correct words: \", correct_num)\n",
    "#     print(\"unknown words: \", unknown_num)\n",
    "    print(\"total word: \", total_num)\n",
    "    print(\"spend time: \", spend_time, \"seconds\")\n",
    "    print(\"accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct words:  190\n",
      "total word:  270\n",
      "spend time:  5.1764140129089355 seconds\n",
      "accuracy:  0.7037037037037037\n"
     ]
    }
   ],
   "source": [
    "auto_correct(words_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct words:  267\n",
      "total word:  823\n",
      "spend time:  50.30043029785156 seconds\n",
      "accuracy:  0.3244228432563791\n"
     ]
    }
   ],
   "source": [
    "auto_correct(words_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The accuracy and time overhead of our model on two different test sets respectively are 60.7%, 23.6% and 0.07s, 0.22s. The accuracy and time overhead of using python open source function \"autocorrect.spell\" on two different test sets respectively are 70.1%, 32.4% and 5.18s, 50.30s. Although the accuracy of \"autocorrect.spell\" is higher than proposed model with 9.4% and 8.8% respectively, the time it takes are 74 and 228 times in two different datasets. This shows that the proposed model is effective to some conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed  Improvements\n",
    "\n",
    "How can we achieve better results ? Let us look back at the three factors in the probability model: (1) $P(c)$; (2) $P(w|c)$; and (3) $argmax_{c}$. The examples of the program giving the wrong answer start, look at these three factors, what we still ignore ?\n",
    "\n",
    "1. $P(c)$. There are two problems that cause the final misidentification. One of the most serious factors is the unknown word. In above two datasets, there are a total of 10 and 35 unknown words. These unknown words account for about 3.7% and 4% respectively. This problem can be solved by adding more corpus. The second factor that can lead to errors is that both words appear in our dictionary, but the one with the highest probability of our selection is not what the user wants.\n",
    "\n",
    "2. $P(w|c)$. The second question is the priority order of the candidate words. So far, we have used a very simple model: the shorter the distance, the greater the probability.\n",
    "\n",
    "3. When we correct words, we don’t consider the contextual relationship. Because in many cases, it is difficult to make decisions based solely on the word itself. The word itself is in the dictionary, but in the context it should be corrected to another word."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
